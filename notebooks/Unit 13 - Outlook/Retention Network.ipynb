{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Challenges\n",
    "Retentive Network (RetNet) is a foundational architecture proposed for large language models in the paper [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621). This architecture is designed to address key challenges in the realm of large-scale language modeling: training parallelism, low-cost inference, and good performance.\n",
    "\n",
    "* **Training parallelism**: During the training, all of the input tokens are processed at the same time, utilizing the GPUs parallel processing power. One example of this is the Transformer architecture; where the self-attention inside the decoder allows token generation that does not depend on the previously generated output. \n",
    "* **Low-cost inference**: During the inference, the cost does not scale with sequence length. One example of this is the Recurrent Neural Network (RNN) architecture; where it uses simple and cheap operation like matrix multiplication to process one token at each time step.\n",
    "* **Good performance**: Transformer and RNN are both excellent, but Transformer has high-cost inference and RNN's training is not parallelizable. On the other hand, linear transformers are parallelizable and its inference is made cheap by sequential processing, but it has poor performance.\n",
    "\n",
    "RetNet addresses all of these challenges thanks to its multi-scale retention mechanism, which will be explained below.\n",
    "\n",
    "Read more about these explanations [here](https://medium.com/ai-fusion-labs/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetNet Architecture\n",
    "\n",
    "<center><img src=\"Retention_imgs/RetNet Architecture.jpg\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "\n",
    "Retentive Network has a similar architecture to the Transformer's encoder, but with a few differences:\n",
    "    <ol>\n",
    "    <li> The encoder precedes the Feed Forward Network (FFN) and token mixer layer (multi-scale retention (MSR) layer).\n",
    "    <li> The multi-scale retention layer (the proposed method) replaces the multi-head attention layer.\n",
    "    </ol>\n",
    "    \n",
    "The input sequence ${x}^{|x|}_{i=1}$ is transformed to vectors by a word embedding layer and then packed to form a matrix $X^0=[x_1, \\cdot\\cdot\\cdot, x_{|x|}] \\in \\mathbb{R}^{|x|\n",
    "\\times d_{model}}$. The computation of the output of the $l$-th layer is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Y^l &= \\text{MSR}(\\text{LN}(X^l)) + X^l \\\\\n",
    "    X^{l+1} &= \\text{FFN}(\\text{LN}(Y^l) + Y^l\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{LN}$ is LayerNorm and $\\text{FFN}$ is the feed forward network computed as $\\text{FFN}(X) = \\text{gelu}(XW_1)W_2$ where $W_1$ and $W_2$ are learnable parameters. \n",
    "\n",
    "Read more:\n",
    "- [Embedding layer](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n",
    "- [gelu](https://arxiv.org/pdf/1606.08415v5.pdf)\n",
    "- [LayerNorm](https://arxiv.org/abs/1607.06450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale Retention \n",
    "\n",
    "<center><img src=\"Retention_imgs/Multi Scale Retention.jpg\" style=\"height: 800px; width:auto;\"></center>\n",
    "\n",
    "The detail of the computation of the multi-scale retention layer is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma &= 1-2^{-5-arange(0,h)} \\\\\n",
    "    head_i &= \\text{Retention}(X, \\gamma_i) \\\\\n",
    "    Y &= GroupNorm_h(Concat(head_1, \\cdot\\cdot\\cdot, head_h)) \\\\\n",
    "    \\text{MSR}(X) &= (swish(XW_G) \\odot Y)W_O\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $h=\\frac{d_{model}}{d}$ is the number of heads, $d$ is the head dimension, $arange(0,h)$ is the range of integers from 0 to $h$, $\\gamma$ is the retention scale, $head_i$ is the output of the $i$-th head, $GroupNorm_h$ is the group normalization applied on each head, $\\odot$ is the element-wise multiplication, $W_G$ and $W_O$ are learnable parameters, and $swish$ is the swish activation function.\n",
    "\n",
    "Let's consider a case like in the figure above, where $h=3$, $|x|=2$, and $d_{model}=4$ (the dimension of the head is usually an integer as $d_{model}$ is the multiplication $d$. Here, $d_{model}=4$ is used only for illustration purpose). Since there are three heads, there will be three different value of gamma $\\gamma_1=1-2^{-5-0}=0.96875$, $\\gamma_2=1-2^{-5-1}=0.9375$, and $\\gamma_3=1-2^{-5-2}=0.875$ applied on each head respectively. However, these gammas are fixed and identical among different layers.\n",
    "\n",
    "Then the input will go through each retention head. Each head's output will be fed to the GroupNorm layer. The GroupNorm layer is applied on each head separately because the heads use multiple $\\gamma$, resulting each head has diferent variance statistics that need to be normalized. After that, the results are concatenated and element-wise multiplied with the output of the swish gate to increase the non-linearity of the retention layers. Finally, the output is projected by multipliying it with $W_O$ so that the output has the same dimension as the input, which is $2 \\times 4$.\n",
    "\n",
    "Read more:\n",
    "- [GroupNorm](https://arxiv.org/abs/1803.08494)\n",
    "- [swish](https://arxiv.org/pdf/1710.05941v1.pdf?source=post_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Block\n",
    "<center><img src=\"Retention_imgs/Retention Block.jpg\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "Retention block is the core of the multi-scale retention layer. It has three representations where the model shifts the representation according to the phase (training or inference). The three representations are:\n",
    "\n",
    "1. **Parallel Representation**\n",
    "2. **Recurrent Representation**\n",
    "3. **Chunkwise Recurrent Representation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Representation\n",
    "<center><img src=\"Retention_imgs/Parallel Representation.png\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "The parallel representation is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "Q=\\left(X W_Q\\right) \\odot \\Theta, \\quad K=\\left(X W_K\\right) \\odot \\bar{\\Theta}, \\quad V=X W_V \\\\\n",
    "\\Theta_n=e^{i n \\theta}, \\quad D_{n m}= \\begin{cases}\\gamma^{n-m}, & n \\geq m \\\\\n",
    "0, & n<m\\end{cases} \\\\\n",
    "\\operatorname{Retention}(X)=\\left(Q K^{\\top} \\odot D\\right) V\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where $\\Theta \\in \\mathbb{R}^{|x|\\times|x|}$ is the positional embedding matrix and $\\bar{\\Theta}$ is its conjugate; $D \\in \\mathbb{R}^{|x|\\times|x|}$ is the decay matrix where it combines causal masking and exponential decay along relative distance between tokens; $W_Q$, $W_K$, and $W_V$ are learnable parameters; and $\\gamma$ is the exponential decay (this $\\gamma$ is similar as mentioned above).\n",
    "\n",
    "#### Positional Embedding Matrix ($\\Theta$)\n",
    "<center><img src=\"Retention_imgs/Positional Embedding.png\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "From [Euler's formula](https://en.wikipedia.org/wiki/Euler%27s_formula) of $e$:\n",
    "\n",
    "$$\n",
    "e^{i \\theta}=\\cos \\theta+i \\sin \\theta\n",
    "$$\n",
    "\n",
    "The role of this matrix is to give the query $Q$ and key $K$ matrices a sense of their relative position. Since the pair vectors from both matrices will be multiplied through the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (matrix multiplication), the positional embedding of each vector becomes:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Theta_n &= e^{i n \\theta} \\\\\n",
    "    &= \\cos(n\\theta) + i\\sin(n\\theta) \\\\\n",
    "    \\bar{\\Theta}_n &= e^{-i n \\theta} \\\\\n",
    "    &= \\cos(n\\theta) - i\\sin(n\\theta) \\\\\n",
    "\n",
    "    \\Theta_n  \\bar{\\Theta}_m &= (\\cos(n\\theta) + i\\sin(n\\theta))(\\cos(m\\theta) - i\\sin(m\\theta)) \\\\\n",
    "    &= \\cos(n\\theta)\\cos(m\\theta) + \\sin(n\\theta)\\sin(m\\theta) + i(\\cos(n\\theta)\\sin(m\\theta) - \\sin(n\\theta)\\cos(m\\theta)) \\\\\n",
    "    &= \\cos(n\\theta - m\\theta) + i\\sin(n\\theta - m\\theta) \\\\\n",
    "    &= e^{i(n-m)\\theta} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "which results in the difference between the angle of the two vectors. Let's consider a case where $m=n=1$, then their products will be $e^{i(1-1)\\theta}=1$ which means that the two vectors have the same direction. This results the value of $1$ along the diagonal part of the $QK^T$ matrix. On the other hand, if $m=1$ and $n=2$, then their products will be $e^{i(2-1)}\\theta=e^{i\\theta}$ which means that there is phase shift and its value is less than one. The further the distance between the two vectors, the more the phase shift and the smaller the value until it becomes zero when the two vectors are orthogonal (red arrows shown in the figure above).\n",
    "\n",
    "#### Decay Matrix ($D$)\n",
    "<center><img src=\"Retention_imgs/Decay Matrix.png\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_{n m} &= \\begin{cases}\\gamma^{n-m}, & n \\geq m \\rightarrow \\text{exponential decay} \\\\\n",
    "    0, & n<m \\rightarrow \\text{causal masking} \n",
    "    \\end{cases} \n",
    "\\end{aligned}\n",
    "$$\n",
    "The decay matrix plays role to apply the causal mask and exponential decay along the relative distance between tokens. The causal masking is similar as the masking attention in self-attention of Transformer, this ensures that the model does not look into the future (this is why the tokens can be processed at the same time aka parallel). On the other hand, the exponential decay ensures the further a token is from the current token, the less important it is by applying weight to the far tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, :, ::2]\n",
    "    x2 = x[:, :, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "def theta_shift(x, sin, cos):\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"swish\":\n",
    "        return F.silu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def MultiwayWrapper(args, module, dim=1):\n",
    "    if args.multiway:\n",
    "        return MultiwayNetwork(module, dim=dim)\n",
    "    return module\n",
    "\n",
    "class MultiwayNetwork(nn.Module):\n",
    "    def __init__(self, module, dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.A = module\n",
    "        self.B = copy.deepcopy(module)\n",
    "        self.B.reset_parameters()\n",
    "        self.split_position = -1\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.split_position == -1:\n",
    "            return self.A(x, **kwargs)\n",
    "        if self.split_position == 0:\n",
    "            return self.B(x, **kwargs)\n",
    "        x1, x2 = torch.split(\n",
    "            x,\n",
    "            [self.split_position, x.size(self.dim) - self.split_position],\n",
    "            dim=self.dim,\n",
    "        )\n",
    "        # x1, x2 = x[:self.split_position], x[self.split_position:]\n",
    "        y1, y2 = self.A(x1, **kwargs), self.B(x2, **kwargs)\n",
    "        return torch.cat([y1, y2], dim=self.dim)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(dim))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        if self.weight is not None:\n",
    "            output = output * self.weight\n",
    "        return output\n",
    "\n",
    "class MultiScaleRetention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        embed_dim,\n",
    "        value_dim,\n",
    "        num_heads,\n",
    "        gate_fn=\"swish\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.embed_dim = embed_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.value_dim // num_heads\n",
    "        self.key_dim = self.embed_dim // num_heads\n",
    "        self.scaling = self.key_dim ** -0.5\n",
    "        \n",
    "        self.gate_fn = get_activation_fn(activation=str(gate_fn))\n",
    "\n",
    "        self.q_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "        self.k_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "        self.v_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "        self.g_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "        \n",
    "        self.out_proj = MultiwayWrapper(args, nn.Linear(value_dim, embed_dim, bias=False))\n",
    "\n",
    "        self.group_norm = MultiwayWrapper(args, RMSNorm(self.head_dim, eps=args.layernorm_eps, elementwise_affine=False))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.g_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight, gain=2 ** -1)\n",
    "\n",
    "    def parallel_forward(self, qr, kr, v, mask):\n",
    "        bsz, tgt_len, embed_dim = v.size()\n",
    "\n",
    "        vr = v.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        qk_mat = qr @ kr.transpose(-1, -2) # bsz * m * tgt_len * tgt_len\n",
    "        qk_mat = qk_mat * mask\n",
    "        # invariant after normalization\n",
    "        qk_mat = qk_mat / qk_mat.detach().abs().sum(dim=-1, keepdim=True).clamp(min=1, max=5e4)\n",
    "        output = torch.matmul(qk_mat, vr)\n",
    "        output = output.transpose(1, 2)\n",
    "        return output\n",
    "\n",
    "    def recurrent_forward(\n",
    "        self,\n",
    "        qr, kr, v,\n",
    "        decay,\n",
    "        incremental_state\n",
    "    ):\n",
    "        bsz = v.size(0)\n",
    "\n",
    "        v = v.view(bsz, self.num_heads, self.head_dim, 1)\n",
    "        kv = kr * v\n",
    "        if \"prev_key_value\" in incremental_state:\n",
    "            prev_kv = incremental_state[\"prev_key_value\"]\n",
    "            prev_scale = incremental_state[\"scale\"]\n",
    "            scale = prev_scale * decay + 1\n",
    "            kv = prev_kv * (prev_scale.sqrt() * decay / scale.sqrt()).view(self.num_heads, 1, 1) + kv / scale.sqrt().view(self.num_heads, 1, 1)\n",
    "            # kv = prev_kv * decay.view(self.num_heads, 1, 1) + kv\n",
    "        else:\n",
    "            scale = torch.ones_like(decay)\n",
    "\n",
    "        incremental_state[\"prev_key_value\"] = kv\n",
    "        incremental_state[\"scale\"] = scale\n",
    "\n",
    "        output = torch.sum(qr * kv, dim=3)\n",
    "        return output\n",
    "    \n",
    "    def chunk_recurrent_forward(\n",
    "        self,\n",
    "        qr, kr, v,\n",
    "        inner_mask\n",
    "    ):\n",
    "        mask, cross_decay, query_inner_decay, value_inner_decay = inner_mask\n",
    "        bsz, tgt_len, embed_dim = v.size()\n",
    "        chunk_len = mask.size(1)\n",
    "        num_chunks = tgt_len // chunk_len\n",
    "\n",
    "        assert tgt_len % chunk_len == 0\n",
    "\n",
    "        qr = qr.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(1, 2)\n",
    "        kr = kr.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(1, 2)\n",
    "        v = v.view(bsz, num_chunks, chunk_len, self.num_heads, self.head_dim).transpose(2, 3)\n",
    "\n",
    "        kr_t = kr.transpose(-1, -2)\n",
    "\n",
    "        qk_mat = qr @ kr_t # bsz * num_heads * chunk_len * chunk_len\n",
    "        qk_mat = qk_mat * mask\n",
    "        inner_scale = qk_mat.detach().abs().sum(dim=-1, keepdim=True).clamp(min=1)\n",
    "        qk_mat = qk_mat / inner_scale\n",
    "        inner_output = torch.matmul(qk_mat, v) # bsz * num_heads * num_value_heads * chunk_len * head_dim\n",
    "        \n",
    "        # reduce kv in one chunk\n",
    "        kv = kr_t @ (v * value_inner_decay)\n",
    "\n",
    "        kv_recurrent = []\n",
    "        cross_scale = []\n",
    "        kv_state = torch.zeros(bsz, self.num_heads, self.key_dim, self.head_dim).to(v)\n",
    "        kv_scale = torch.ones(bsz, self.num_heads, 1, 1).to(v)\n",
    "        \n",
    "        # accumulate kv by loop\n",
    "        for i in range(num_chunks):\n",
    "            kv_recurrent.append(kv_state / kv_scale)\n",
    "            cross_scale.append(kv_scale)\n",
    "            kv_state = kv_state * cross_decay + kv[:, i]\n",
    "            kv_scale = kv_state.detach().abs().sum(dim=-2, keepdim=True).max(dim=-1, keepdim=True).values.clamp(min=1)\n",
    "            \n",
    "        kv_recurrent = torch.stack(kv_recurrent, dim=1)\n",
    "        cross_scale = torch.stack(cross_scale, dim=1)\n",
    "        \n",
    "        all_scale = torch.maximum(inner_scale, cross_scale)\n",
    "        align_inner_scale = all_scale / inner_scale\n",
    "        align_cross_scale = all_scale / cross_scale\n",
    "\n",
    "        cross_output = (qr * query_inner_decay) @ kv_recurrent\n",
    "        output = inner_output / align_inner_scale + cross_output / align_cross_scale\n",
    "        # output = inner_output / cross_scale + cross_output / inner_scale\n",
    "\n",
    "        output = output.transpose(2, 3)\n",
    "        return output\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        rel_pos,\n",
    "        chunkwise_recurrent=False,\n",
    "        incremental_state=None\n",
    "    ):\n",
    "        bsz, tgt_len, _ = x.size()\n",
    "        (sin, cos), inner_mask = rel_pos\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        g = self.g_proj(x)\n",
    "\n",
    "        k *= self.scaling\n",
    "        q = q.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "\n",
    "        qr = theta_shift(q, sin, cos)\n",
    "        kr = theta_shift(k, sin, cos)\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            output = self.recurrent_forward(qr, kr, v, inner_mask, incremental_state)\n",
    "        elif chunkwise_recurrent:\n",
    "            output = self.chunk_recurrent_forward(qr, kr, v, inner_mask)\n",
    "        else:\n",
    "            output = self.parallel_forward(qr, kr, v, inner_mask)\n",
    "        \n",
    "        output = self.group_norm(output).reshape(bsz, tgt_len, self.head_dim * self.num_heads)\n",
    "\n",
    "        output = self.gate_fn(g) * output\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Going line by line in the `forward` function:\n",
    "**Variables**\n",
    "These are important variables that are going to be used along the computation:\n",
    "1. Batch size (bsz): The batch size of the input.\n",
    "2. Target len / Sequence len (tgt_len): The length of the sequence.\n",
    "3. (sin, cos): Angle for positional embedding.\n",
    "4. inner_mask: Masking matrix; depends on the mode used, it can also contain other constants.\n",
    "```python\n",
    "bsz, tgt_len, _ = x.size()\n",
    "(sin, cos), inner_mask = rel_pos\n",
    "```\n",
    "\n",
    "**Key, Query, Value**\n",
    "Obtain the key, query, and value representation of the input by multiplying the input with learnable matrices.\n",
    "```python\n",
    "q = self.q_proj(x)\n",
    "k = self.k_proj(x)\n",
    "v = self.v_proj(x)\n",
    "g = self.g_proj(x)\n",
    "```\n",
    "They are declared in the `__init__` function\n",
    "```python\n",
    "self.q_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "self.k_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "self.v_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "self.g_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "```\n",
    "\n",
    "**Multi-Head**\n",
    "Dividing the query and key matrices into several heads (similar to Multi-Head Attention).\n",
    "```python\n",
    "k *= self.scaling\n",
    "q = q.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "k = k.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "```\n",
    "The number of heads and key dimension are declared in the `__init__` function.\n",
    "```python\n",
    "self.num_heads = num_heads\n",
    "self.head_dim = self.value_dim // num_heads\n",
    "self.key_dim = self.embed_dim // num_heads\n",
    "```\n",
    "The multi-head operation will also be applied to the value matrix, but it is implemented inside the represenation calculation (parallel, recurrent, chunkwise recurrent).\n",
    "\n",
    "**Positional Embedding**\n",
    "Add the Extrapolatable Position Embedding [XPos](https://arxiv.org/abs/2212.10554).\n",
    "```python\n",
    "qr = theta_shift(q, sin, cos)\n",
    "kr = theta_shift(k, sin, cos)\n",
    "```\n",
    "where `theta_shift` function is defined outside the class.\n",
    "```python\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, :, ::2]\n",
    "    x2 = x[:, :, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "def theta_shift(x, sin, cos):\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "```\n",
    "\n",
    "**Representation Calculation**\n",
    "Multi-Scale Retention performs computation according to the condition.\n",
    "```python\n",
    "if incremental_state is not None:\n",
    "    output = self.recurrent_forward(qr, kr, v, inner_mask, incremental_state)\n",
    "elif chunkwise_recurrent:\n",
    "    output = self.chunk_recurrent_forward(qr, kr, v, inner_mask)\n",
    "else:\n",
    "    output = self.parallel_forward(qr, kr, v, inner_mask)\n",
    "```\n",
    "where each representation computation is declared inside the MultiScaleRetention class (will be explained later).\n",
    "\n",
    "**Normalization, Gating, and Projection**\n",
    "Perform normalization, apply gating function, and do projection.\n",
    "```python\n",
    "output = self.group_norm(output).reshape(bsz, tgt_len, self.head_dim * self.num_heads)\n",
    "output = self.gate_fn(g) * output\n",
    "output = self.out_proj(output)\n",
    "return output\n",
    "```\n",
    "where each function is declared in the `__init__` function\n",
    "```python\n",
    "self.gate_fn = get_activation_fn(activation=str(gate_fn))\n",
    "self.out_proj = MultiwayWrapper(args, nn.Linear(value_dim, embed_dim, bias=False))\n",
    "self.group_norm = MultiwayWrapper(args, RMSNorm(self.head_dim, eps=args.layernorm_eps, elementwise_affine=False))\n",
    "```\n",
    "and `get_activation_fn` is declared outside the class\n",
    "```python\n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"swish\":\n",
    "        return F.silu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
